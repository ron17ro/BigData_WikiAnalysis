import java.io.IOException;
import org.apache.hadoop.io.DoubleWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

/**
 * 
 * Task2WikiStatsReducer will take the pairs generated by the Combiner class
 *  in the form <Text, Text> <language, averagePageViews_numberOfPages> 
 * and will compute the final page views average
 *
 */

public class Task2WikiStatsReducer extends Reducer<Text, Text, Text, DoubleWritable> {

	@Override
	public void reduce(Text key, Iterable<Text> values, Context context)
			throws IOException, InterruptedException {
		
		System.out.println(" In Reducer now!");
		
		DoubleWritable result = new DoubleWritable();
		Double pageViewsCount = 0D;		
		int counter = 0;
		
		
		for (Text value : values) {
			/*
			 * see if the file was already aggregated in a pair <key, a_b> by checking the presence
			 * of the "_" in the value. Recompute the total page views
			 * by multiplying a*b and adding the value to the pageViewCount
			 *  
			 * If the value wasn't already aggregated then parse the value
			 *  and aggregate it to the total pageViewsCount 
			 */
			if(value.toString().contains("_")) {
				String[] averageTokens = value.toString().split("_");
				counter += Integer.parseInt(averageTokens[1].toString());
				pageViewsCount += Double.parseDouble(averageTokens[0].toString()) * counter;
				
			}else {					
				pageViewsCount += Double.parseDouble(value.toString());
				counter++;
			}
					
		}
		/*
		 * compute the average and leave only 2 decimal places by formating the average  
		 */
		result.set(Double.parseDouble(String.format( "%.2f", pageViewsCount / counter)));				
		
		context.write(key, result);
	
		
	}

}
